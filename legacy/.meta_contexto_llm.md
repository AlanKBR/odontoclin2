# Plano de Implementação: Contexto de Conversa para Modelos LLM Locais (OdontoClin)

## Objetivo
Permitir que modelos LLM locais (ex: BioMistral) utilizem o histórico de conversa para respostas contextualizadas, com controle de limite de histórico pelo usuário via frontend.

---

## 1. Armazenamento do Histórico
- **Opção escolhida para protótipo:** Sessão Flask (por usuário).
- **Alternativas:** Banco de dados (persistente), arquivo temporário, LocalStorage (frontend).
- **Prós/Contras:**
  - Sessão: simples, não persiste após restart.
  - Banco: persistente, mais complexo.

## 2. Backend: Montagem do Contexto
- Recuperar histórico da sessão do usuário.
- Concatenar histórico no formato esperado pelo modelo local (ex: "Usuário: ...", "Assistente: ...").
- Limitar o histórico ao valor definido pelo usuário (slider frontend).
- Incluir sempre o system prompt no início.
- Salvar cada nova interação no histórico após resposta do modelo.

## 3. Frontend: Slider de Limite de Histórico
- Adicionar slider para o usuário escolher o número máximo de mensagens do histórico (ex: 2 a 30).
- Exibir valor atual do slider.
- Enviar valor do slider ao backend junto com cada requisição de chat.

## 4. Integração
- Backend deve receber e aplicar o limite do slider ao montar o contexto.
- Frontend deve garantir envio do valor do slider.

## 5. Testes
- Testar diferentes limites de histórico.
- Validar persistência e limpeza do histórico.
- Garantir que o histórico não ultrapasse o limite de tokens do modelo.

## 6. Documentação
- Manter este arquivo atualizado durante a implementação.
- Adicionar decisões e observações relevantes.

---

## Status
- [ ] Armazenamento do histórico definido
- [ ] Backend preparado para montar contexto
- [ ] Slider de limite de histórico no frontend
- [ ] Integração frontend-backend
- [ ] Testes e validação

> Este arquivo serve como referência para continuidade da implementação por qualquer agente Copilot.
